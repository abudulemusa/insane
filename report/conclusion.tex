\chapter{Limitations and Future Work}
\label{chap:conclusion}
\section{Concurrency}
Considering arbitrary interleaving would considerably complicate our analysis.
Indeed, the concepts of previous value or strong update are not intuitively
defined in the presence of concurrency. For this reason, we decided to ignore
concurrency. Since Scala promotes an actor model for implementing concurrent
programs, we believe that it is reasonable to consider a sequential execution
of the code. This would be safe if objects passed as messages are not
arbitrarily modified on both ends, which is a property that we could check
using our analysis, but currently do not.

\section{Exceptions}
It is worth noting that we currently do not handle exceptions. In effect, we
ignore exception handlers (e.g. \lstinline{try/catch} blocks), and assume that
\lstinline{throw} statements redirects the flow directly to the end of the
procedure. Not only this way of handling is not precise, but it is also not
valid in theory. However, even though we think that this should be fixed in
future versions, we believe that improving the handling of exceptions should
only have a limited impact on the resulting precision in terms of effects and
alias information.

\section{Nullness Analysis}
Given that our analysis performs strong updates whenever possible, we are able
to obtain a relatively precise \emph{nullness} analysis for free. Indeed, if during
a call $r = obj.foo$ or a field access $obj.f$, we have that $NNode \in
nodes(obj)$, we can raise a warning about a potential null dereference that
could cause the program to crash entirely. However, this remains a pretty
primitive analysis, as it currently does not take \emph{if} conditions into
account. As a result, the following code would generate a spurious warning:
\begin{lstlisting}
if (a != null) {
    a.foo()
}
\end{lstlisting}
It would be interesting to improve the precision of this analysis in order to
potentially spot bugs in existing Scala applications.

It is worth noting that Scala implicitly discourages the use of \emph{null} to
indicate the absence of values. Rather, it defines an \emph{Option[T]}
datastructure with two subtypes $Some[T](val)$ or $None$. However,
nothing prevents a developer to blindly call Option's \emph{get} method, which
in the case of $None$ is equivalent to a null-dereference. We could certainly
apply the same principles here in order to detect such cases.

\section{Higher Order Functions}
The presence of Higher Order Functions (HOF) complicates our analysis and
compromise its precision. In Scala, HOFs are represented as objects, instances
of FunctionX classes where X is the number of arguments the function has. To
illustrate, we consider a use of a HOF:
\begin{lstlisting}
def test() = {
    plop(42, _ + 1)
}
def plop(i: Int, f: Int => Int): Int = f(i)
\end{lstlisting}
We have here a function named \lstinline{plop} which applies the function $f$ passed
as second argument to its first argument. In \lstinline{test} we call that method
with $42$, and the function incrementing its argument by one. The result of
\lstinline{test} should thus be $43$. At our phase, the compiler already
translated that code to:
\begin{lstlisting}
def test(): Int = {
    plop(42, (new Test$$anonfun$test$1(): Function1))
}
def plop(i: Int, f: Function1): Int = f.apply$mcII$sp(i);

class Test$$anonfun$test$1 extends
  scala.runtime.AbstractFunction1$mcII$sp with Serializable {

  final def apply(x$1: Int): Int =
    Test$$anonfun$test$1.this.apply$mcII$sp(x$1);

  def apply$mcII$sp(v1: Int): Int =
    v1.+(1);

  final def apply(v1: java.lang.Object): java.lang.Object =
    scala.Int.box(
      Test$$anonfun$test$1.this.apply(scala.Int.unbox(v1))
    );
}

\end{lstlisting}
As we can see, the compiler transformed the type \lstinline{Int => Int} into
the general type \lstinline{Function1}. It also transformed the closure
\lstinline{_ + 1} into a class defining, among other things, a
\lstinline{apply$mcII$sp} method which is the specialized name of the method
for \lstinline{Int => Int}. The call to \lstinline{f} is transformed into a
method call to that \lstinline{apply$mcII$sp}.

If we recall how type analysis is performed for arguments of methods, we can
immediately see a problem in the presence of HOFs. Indeed, the method
\lstinline{plop} takes an argument \lstinline{f} of type \lstinline{Function1}
which is the super type of all functions of one argument. The runtime types
calculated for \lstinline{f} thus includes \emph{all} closures of one argument,
including ones with incompatible types. As a result, any call using
\lstinline{f} as a receiver potentially targets every defined closures.

In order to address that issue, we could implement the following three
techniques:

\subsection{Exploring Type History}
The main reason why types are generalized is that our analysis runs
after the \emph{errasure} phase, which is responsible of removing type
information that cannot remain at runtime because of JVM limitations (mostly
generic types). Thankfully, the compiler keeps an history of the types
associated to each symbol. We could thus recollect the type of the arguments
prior to the \emph{errasure} phase, allowing us to limit the targets to methods
of compatible type.

\subsection{Selective Analysis Inlining}
Even though the previous technique would help eliminate many spurious targets,
it would remain highly imprecise. Figure~\ref{fig:con:inl} provides an example
illustrating the imprecision. Assuming that the closures defined in
\lstinline{test1} and \lstinline{test2} are the only instance of
\lstinline{Function1}, the effects inferred for \lstinline{plop} is the
combination of the effects of the two closures. As a result, we infer that
\lstinline{test1} writes to the field $a$ even though it does not.

\begin{figure}[h]
    \centering
\begin{lstlisting}
class Test {
    var a: Int = 42

    def test1() = {
        plop(_ + 1)
    }

    def test2() = {
        plop(x => a += 1; a)
    }

    def plop(f: Int => Int) = f(42)
}
\end{lstlisting}
    \caption{Imprecise effects inference}
    \label{fig:con:inl}
\end{figure}

By selectively inlining the analysis of the method \lstinline{plop} in
\lstinline{test1} and \lstinline{test2}, we could refine the type of the
argument from $Function1$ to the exact type of the class generated for each
closure. Type analysis would then naturally infer that the
\lstinline{f.apply$mcII$sp} call in \lstinline{plop} has only one target.  As a
result, the effects of \lstinline{test1} and \lstinline{test2} would be
inferred with respect to the closure they define and use.

\subsection{Graph-based Delaying of Method Calls}
Instead of inlining the entire analysis of a method, we explored ways to
generate graphs in which certain calls would remain unresolved. The main idea
is that in the presence of an imprecise function call, we could replace the
call by a special node indicating a method call, and "wait" until the receiver
gets refined to actually apply the method call. This refinement would be done
automatically by node remapping, given that we use a graph-based type analysis.
We could thus keep the overall modularity of our analysis, and decide to delay
problematic method calls, which would include but not be limited to HOFs.

Even though this idea is appealing, it is yet still unclear how to manage those
delayed calls while preserving soundness. The main difficulties that such a
technique would bring are of temporal nature. Assumptions of "happened before"
no longer hold in the presence of delayed calls, causing precision loss:
\begin{enumerate}
    \item Strong updates could no longer be applied at various places, i.e. in
    the delayed method call.
    \item Load nodes that were related to those delayed calls cannot be resolved
    before the call itself.
\end{enumerate}
Due to time restrictions, we could not go further with the development of this
promising feature.

%\section{Incremental Analysis}

\section{Conclusion}
We presented \insane, an interprocedural pointer and effect analysis for Scala.
Even though the analysis considers the entire program, it is compositional and
allows intermediate results to be stored for efficient re-use. While it
is not incremental now, we are let to believe that modifications required for
it to be incremental would be small, given that it is already compositional. In
overall, we provided an efficient and precise tool for effect and pointer
analysis, an important basic block enabling future and more sophisticated
analyses for Scala.
